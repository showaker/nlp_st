## Q：位置编码相关positonal encoding

## Q：自注意力机制数学表达式是什么
A：自注意力数学表达式为：
> ![image](https://github.com/user-attachments/assets/195ed14c-2fcb-4c3a-83aa-03e5dc360a6d)

其中`QKV`为查询、键、值矩阵，`dk`是键向量维度，这里通过将查询和键的点积除以根号dk进行缩放，然后用softmax获取权重，最后将权重用于值向量的加权求和。
## Q: 为什么计算时要除以根号dk
A: 
## Q：如何理解词嵌入Embedding
A：利用矩阵乘法实现降维，避免one-hot需要的高纬度矩阵。也可以用来升维，连接高低维运算。
在机器学习和NLP中，可以将其理解为一种“向量化”或“向量表示”的技术。
在机器学习中，主要指将离散的高维数据映射到低维的连续向量空间，生成实数向量，用来捕捉原始数据的潜在关系和结构。
在NLP中，Embedding技术（如word2vec）将单词或短语映射为向量，使语义上相似的单词在向量空间中位置相近。
词嵌入是将单词映射为数值向量，捕捉单词间的语义和句法关系，为NLP任务提供有效的特征表示。通过预测单词上下文（word2vec）或全局词频统计（GloVe）来学习，也可以使用DNN捕捉更复杂的语言特性。




